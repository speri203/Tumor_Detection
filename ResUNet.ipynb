{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResUNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_pMnkBVkKG2",
        "outputId": "b70ffb34-2244-40bc-8e99-619a634c3a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT2mc4Z6kL9Z",
        "outputId": "6b92886d-1e26-4d8f-f16b-9f7a323aac86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd 'filepath of dataI'"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filepath of data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzYK4aqOkL_4"
      },
      "source": [
        "## Importing required modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import cv2\n",
        "from skimage import io\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSWE96-4kMCL",
        "outputId": "e49736da-ef27-4f31-b6cc-364af4be84d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = pd.read_csv('data_mask.csv')\n",
        "df.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3929, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuK4gLWLkMEd",
        "outputId": "1e54721e-62a3-46a3-922c-adb1b029f746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "mask_df = df[df['mask'] == 1]\n",
        "df['mask'] = df['mask'].apply(lambda x: str(x))\n",
        "mask_df.head()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>mask_path</th>\n",
              "      <th>mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>TCGA_DU_5872_19950223</td>\n",
              "      <td>TCGA_CS_5393_19990606/TCGA_CS_5393_19990606_5.tif</td>\n",
              "      <td>TCGA_CS_5393_19990606/TCGA_CS_5393_19990606_5_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>TCGA_DU_5874_19950510</td>\n",
              "      <td>TCGA_HT_7680_19970202/TCGA_HT_7680_19970202_5.tif</td>\n",
              "      <td>TCGA_HT_7680_19970202/TCGA_HT_7680_19970202_5_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>TCGA_DU_5854_19951104</td>\n",
              "      <td>TCGA_CS_4944_20010208/TCGA_CS_4944_20010208_6.tif</td>\n",
              "      <td>TCGA_CS_4944_20010208/TCGA_CS_4944_20010208_6_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>TCGA_DU_5854_19951104</td>\n",
              "      <td>TCGA_CS_5393_19990606/TCGA_CS_5393_19990606_6.tif</td>\n",
              "      <td>TCGA_CS_5393_19990606/TCGA_CS_5393_19990606_6_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>617</th>\n",
              "      <td>TCGA_DU_5853_19950823</td>\n",
              "      <td>TCGA_HT_7680_19970202/TCGA_HT_7680_19970202_6.tif</td>\n",
              "      <td>TCGA_HT_7680_19970202/TCGA_HT_7680_19970202_6_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                patient_id  ... mask\n",
              "445  TCGA_DU_5872_19950223  ...    1\n",
              "507  TCGA_DU_5874_19950510  ...    1\n",
              "551  TCGA_DU_5854_19951104  ...    1\n",
              "555  TCGA_DU_5854_19951104  ...    1\n",
              "617  TCGA_DU_5853_19950823  ...    1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5soUYZqDkMGz"
      },
      "source": [
        "## splitting data up into training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val = train_test_split(mask_df, test_size = 0.15)\n",
        "X_test, X_val = train_test_split(X_val, test_size = 0.5)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYZSqF7QkMI_",
        "outputId": "1e65bf3b-15cf-407e-b899-dbcb40fbd13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Training ds: {}'.format(X_train.shape))\n",
        "print('Validation ds: {}'.format(X_val.shape))\n",
        "print('Testing ds: {}'.format(X_test.shape))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ds: (1167, 4)\n",
            "Validation ds: (103, 4)\n",
            "Testing ds: (103, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLEuSBC4kMLY"
      },
      "source": [
        "## The input will be the MRI image and the output will be the mask segmentation \n",
        "train_ids = list(X_train.image_path)\n",
        "train_mask = list(X_train.mask_path)\n",
        "\n",
        "val_ids = list(X_val.image_path)\n",
        "val_mask = list(X_val.mask_path)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BFPydMRkMNx"
      },
      "source": [
        "from utilities import DataGenerator\n",
        "\n",
        "training_gen = DataGenerator(train_ids, train_mask)\n",
        "validation_gen = DataGenerator(val_ids, val_mask)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSgCxXnPkMQO"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Activation, Add, UpSampling2D, Concatenate\n",
        "## Building the residual block\n",
        "def residual_block(X, filter):\n",
        "\n",
        "  ## Copying the input\n",
        "  X_copy = X\n",
        "\n",
        "  ## Main block\n",
        "  X = Conv2D(filter, kernel_size = (1, 1), strides = (1, 1), kernel_initializer = 'he_normal')(X)\n",
        "  X = BatchNormalization()(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  X = Conv2D(filter, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = 'he_normal')(X)\n",
        "  X = BatchNormalization()(X)\n",
        "\n",
        "  ## SHhortpath (Due to the dimensions changing with the 2 Conv2D operations)\n",
        "  X_copy = Conv2D(filter, kernel_size = (1, 1), strides = (1, 1), kernel_initializer = 'he_normal')(X_copy)\n",
        "  X_copy = BatchNormalization()(X_copy)\n",
        "\n",
        "  ## Adding both paths together\n",
        "  X = Add()([X, X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgm4QSdikMSu"
      },
      "source": [
        "## Defining the upsampling \n",
        "def upsampling_concat(X, encoder_output):\n",
        "  X = UpSampling2D((2, 2)) (X)\n",
        "  merge = Concatenate()([X, encoder_output])\n",
        "  \n",
        "  return merge"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZkNm8PokMU8"
      },
      "source": [
        "\n",
        "## Building ResUNet\n",
        "input_size = (256, 256, 3)\n",
        "\n",
        "X_input = Input(input_size)\n",
        "\n",
        "## Implementing encoder steps\n",
        "conv_input = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(X_input)\n",
        "conv_input = BatchNormalization()(conv_input)\n",
        "conv_input = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv_input)\n",
        "conv_input = BatchNormalization()(conv_input)\n",
        "conv_input_pooling = MaxPool2D(pool_size= (2, 2))(conv_input)\n",
        "\n",
        "conv2_input = residual_block(conv_input_pooling, 32)\n",
        "conv2_pooling = MaxPool2D(pool_size= (2, 2))(conv2_input)\n",
        "\n",
        "conv3_input = residual_block(conv2_pooling, 64)\n",
        "conv3_pooling = MaxPool2D(pool_size= (2, 2))(conv3_input)\n",
        "\n",
        "conv4_input = residual_block(conv3_pooling, 128)\n",
        "conv4_pooling = MaxPool2D(pool_size= (2, 2))(conv4_input)\n",
        "\n",
        "## Bottleneck \n",
        "conv5_input = residual_block(conv4_pooling, 256)\n",
        "\n",
        "## Transitioning into decoding\n",
        "decode_1 = upsampling_concat(conv5_input, conv4_input)\n",
        "decode_1 = residual_block(decode_1, 128)\n",
        "\n",
        "decode_2 = upsampling_concat(decode_1, conv3_input)\n",
        "decode_2 = residual_block(decode_2, 64)\n",
        "\n",
        "decode_3 = upsampling_concat(decode_2, conv2_input)\n",
        "decode_3 = residual_block(decode_3, 32)\n",
        "\n",
        "decode_4 = upsampling_concat(decode_3, conv_input)\n",
        "decode_4 = residual_block(decode_4, 16)\n",
        "\n",
        "## Adding output convolution\n",
        "conv_output = Conv2D(1, (1,1), padding = 'same', activation = 'sigmoid')(decode_4)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F09j5og5kMXU",
        "outputId": "fbe5b7b0-cd65-49eb-e0f8-ce30c9b6c6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Model architecture\n",
        "model = Model(inputs = X_input, outputs = conv_output)\n",
        "model.summary()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 256, 256, 16) 448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 256, 256, 16) 64          conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 256, 256, 16) 2320        batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 256, 256, 16) 64          conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 16) 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 128, 128, 32) 544         max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 128, 128, 32) 128         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 128, 128, 32) 544         max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 128, 128, 32) 128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 128, 128, 32) 128         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 128, 128, 32) 0           batch_normalization_29[0][0]     \n",
            "                                                                 batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 128, 128, 32) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 32)   0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 64, 64, 64)   2112        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 64, 64, 64)   256         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 64, 64, 64)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 64, 64, 64)   36928       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 64, 64, 64)   2112        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 64, 64, 64)   256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 64, 64, 64)   256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 64, 64, 64)   0           batch_normalization_32[0][0]     \n",
            "                                                                 batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 64, 64, 64)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 128)  8320        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 128)  512         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 128)  8320        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 32, 32, 128)  512         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 32, 128)  512         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 128)  0           batch_normalization_35[0][0]     \n",
            "                                                                 batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 128)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 256)  33024       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 256)  1024        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 256)  590080      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 256)  33024       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 256)  1024        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 256)  1024        conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 16, 256)  0           batch_normalization_38[0][0]     \n",
            "                                                                 batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 256)  0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 256)  0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 384)  0           up_sampling2d_4[0][0]            \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 128)  49280       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 32, 32, 128)  512         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 128)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 32, 32, 128)  147584      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 32, 32, 128)  49280       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 32, 32, 128)  512         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 32, 32, 128)  512         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 32, 32, 128)  0           batch_normalization_41[0][0]     \n",
            "                                                                 batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 128)  0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 128)  0           activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 64, 64, 192)  0           up_sampling2d_5[0][0]            \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 64, 64, 64)   12352       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 64, 64, 64)   256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 64, 64, 64)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 64, 64, 64)   36928       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 64, 64, 64)   12352       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 64, 64, 64)   256         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 64, 64, 64)   256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 64, 64, 64)   0           batch_normalization_44[0][0]     \n",
            "                                                                 batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 64, 64, 64)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 64) 0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 128, 128, 96) 0           up_sampling2d_6[0][0]            \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 128, 128, 32) 3104        concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 128, 128, 32) 128         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 128, 128, 32) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 128, 128, 32) 9248        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 128, 128, 32) 3104        concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 128, 128, 32) 128         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 128, 128, 32) 128         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 128, 128, 32) 0           batch_normalization_47[0][0]     \n",
            "                                                                 batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 128, 128, 32) 0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 32) 0           activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 256, 256, 48) 0           up_sampling2d_7[0][0]            \n",
            "                                                                 batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 256, 256, 16) 784         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 256, 256, 16) 64          conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 256, 256, 16) 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 256, 256, 16) 2320        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 256, 256, 16) 784         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 256, 256, 16) 64          conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 256, 256, 16) 64          conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 256, 256, 16) 0           batch_normalization_50[0][0]     \n",
            "                                                                 batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 256, 256, 16) 0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 256, 256, 1)  17          activation_31[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,210,513\n",
            "Trainable params: 1,206,129\n",
            "Non-trainable params: 4,384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J50FwitkMZw"
      },
      "source": [
        "## Training the model \n",
        "## Will be using custom loss function (tversky)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os \n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "from tensorflow.keras import backend as K\n",
        "  \n",
        "#creating a custom datagenerator:\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, ids , mask, image_dir = './', batch_size = 16, img_h = 256, img_w = 256, shuffle = True):\n",
        "\n",
        "    self.ids = ids\n",
        "    self.mask = mask\n",
        "    self.image_dir = image_dir\n",
        "    self.batch_size = batch_size\n",
        "    self.img_h = img_h\n",
        "    self.img_w = img_w\n",
        "    self.shuffle = shuffle\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    'Get the number of batches per epoch'\n",
        "\n",
        "    return int(np.floor(len(self.ids)) / self.batch_size)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generate a batch of data'\n",
        "\n",
        "    #generate index of batch_size length\n",
        "    indexes = self.indexes[index* self.batch_size : (index+1) * self.batch_size]\n",
        "\n",
        "    #get the ImageId corresponding to the indexes created above based on batch size\n",
        "    list_ids = [self.ids[i] for i in indexes]\n",
        "\n",
        "    #get the MaskId corresponding to the indexes created above based on batch size\n",
        "    list_mask = [self.mask[i] for i in indexes]\n",
        "\n",
        "\n",
        "    #generate data for the X(features) and y(label)\n",
        "    X, y = self.__data_generation(list_ids, list_mask)\n",
        "\n",
        "    #returning the data\n",
        "    return X, y\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    'Used for updating the indices after each epoch, once at the beginning as well as at the end of each epoch'\n",
        "    \n",
        "    #getting the array of indices based on the input dataframe\n",
        "    self.indexes = np.arange(len(self.ids))\n",
        "\n",
        "    #if shuffle is true, shuffle the indices\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.indexes)\n",
        "\n",
        "  def __data_generation(self, list_ids, list_mask):\n",
        "    'generate the data corresponding the indexes in a given batch of images'\n",
        "\n",
        "    # create empty arrays of shape (batch_size,height,width,depth) \n",
        "    #Depth is 3 for input and depth is taken as 1 for output becasue mask consist only of 1 channel.\n",
        "    X = np.empty((self.batch_size, self.img_h, self.img_w, 3))\n",
        "    y = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n",
        "\n",
        "    #iterate through the dataframe rows, whose size is equal to the batch_size\n",
        "    for i in range(len(list_ids)):\n",
        "      #path of the image\n",
        "      img_path = './' + str(list_ids[i])\n",
        "      \n",
        "      #mask path\n",
        "      mask_path = './' + str(list_mask[i])\n",
        "      \n",
        "      #reading the original image and the corresponding mask image\n",
        "      img = io.imread(img_path)\n",
        "      mask = io.imread(mask_path)\n",
        "\n",
        "      #resizing and coverting them to array of type float64\n",
        "      img = cv2.resize(img,(self.img_h,self.img_w))\n",
        "      img = np.array(img, dtype = np.float64)\n",
        "      \n",
        "      mask = cv2.resize(mask,(self.img_h,self.img_w))\n",
        "      mask = np.array(mask, dtype = np.float64)\n",
        "\n",
        "      #standardising \n",
        "      img -= img.mean()\n",
        "      img /= img.std()\n",
        "      \n",
        "      mask -= mask.mean()\n",
        "      mask /= mask.std()\n",
        "      \n",
        "      #Adding image to the empty array\n",
        "      X[i,] = img\n",
        "      \n",
        "      #expanding the dimnesion of the image from (256,256) to (256,256,1)\n",
        "      y[i,] = np.expand_dims(mask, axis = 2)\n",
        "    \n",
        "    #normalizing y\n",
        "    y = (y > 0).astype(int)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prediction(test, model, model_seg):\n",
        "  '''\n",
        "  Predcition function which takes dataframe containing ImageID as Input and perform 2 type of prediction on the image\n",
        "  Initially, image is passed through the classification network which predicts whether the image has defect or not, if the model\n",
        "  is 99% sure that the image has no defect, then the image is labeled as no-defect, if the model is not sure, it passes the image to the\n",
        "  segmentation network, it again checks if the image has defect or not, if it has defect, then the type and location of defect is found\n",
        "  '''\n",
        "\n",
        "  #directory\n",
        "  directory = \"./\"\n",
        "\n",
        "  #Creating empty list to store the results\n",
        "  mask = []\n",
        "  image_id = []\n",
        "  has_mask = []\n",
        "\n",
        "  #iterating through each image in the test data\n",
        "  for i in test.image_path:\n",
        "\n",
        "    path = directory + str(i)\n",
        "\n",
        "    #reading the image\n",
        "    img = io.imread(path)\n",
        "\n",
        "    #Normalizing the image\n",
        "    img = img * 1./255.\n",
        "\n",
        "    #Reshaping the image\n",
        "    img = cv2.resize(img,(256,256))\n",
        "\n",
        "    #Converting the image into array\n",
        "    img = np.array(img, dtype = np.float64)\n",
        "    \n",
        "    #reshaping the image from 256,256,3 to 1,256,256,3\n",
        "    img = np.reshape(img, (1,256,256,3))\n",
        "\n",
        "    #making prediction on the image\n",
        "    is_defect = model.predict(img)\n",
        "\n",
        "    #if tumour is not present we append the details of the image to the list\n",
        "    if np.argmax(is_defect) == 0:\n",
        "      image_id.append(i)\n",
        "      has_mask.append(0)\n",
        "      mask.append('No mask')\n",
        "      continue\n",
        "\n",
        "    #Read the image\n",
        "    img = io.imread(path)\n",
        "\n",
        "    #Creating a empty array of shape 1,256,256,1\n",
        "    X = np.empty((1, 256, 256, 3))\n",
        "\n",
        "    #resizing the image and coverting them to array of type float64\n",
        "    img = cv2.resize(img,(256,256))\n",
        "    img = np.array(img, dtype = np.float64)\n",
        "\n",
        "    #standardising the image\n",
        "    img -= img.mean()\n",
        "    img /= img.std()\n",
        "\n",
        "    #converting the shape of image from 256,256,3 to 1,256,256,3\n",
        "    X[0,] = img\n",
        "\n",
        "    #make prediction\n",
        "    predict = model_seg.predict(X)\n",
        "\n",
        "    #if the sum of predicted values is equal to 0 then there is no tumour\n",
        "    if predict.round().astype(int).sum() == 0:\n",
        "        image_id.append(i)\n",
        "        has_mask.append(0)\n",
        "        mask.append('No mask')\n",
        "    else:\n",
        "    #if the sum of pixel values are more than 0, then there is tumour\n",
        "        image_id.append(i)\n",
        "        has_mask.append(1)\n",
        "        mask.append(predict)\n",
        "\n",
        "\n",
        "  return image_id, mask, has_mask\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "We need a custom loss function to train this ResUNet.So,  we have used the loss function as it is from https://github.com/nabsabraham/focal-tversky-unet/blob/master/losses.py\n",
        "\n",
        "\n",
        "@article{focal-unet,\n",
        "  title={A novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation},\n",
        "  author={Abraham, Nabila and Khan, Naimul Mefraz},\n",
        "  journal={arXiv preprint arXiv:1810.07842},\n",
        "  year={2018}\n",
        "}\n",
        "'''\n",
        "def tversky(y_true, y_pred, smooth = 1e-6):\n",
        "    y_true_pos = K.flatten(y_true)\n",
        "    y_pred_pos = K.flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "    return 1 - tversky(y_true,y_pred)\n",
        "\n",
        "def focal_tversky(y_true,y_pred):\n",
        "    ## IMPORTANT NOTE:\n",
        "    ## It seems that the type of y_true and y_pred are not the same. One is of type\n",
        "    ## float32 and other int64. They need to be cast as the same type for the training\n",
        "    ## to actually work.\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    #print(type(y_pred))\n",
        "    pt_1 = tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.pow((1-pt_1), gamma)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TVpUb0tkMb-"
      },
      "source": [
        "## Compiling the model\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(lr = .05, epsilon = 0.1), loss = focal_tversky, metrics = [tversky])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYdZu3pWkMef"
      },
      "source": [
        "## Early stopping and checkpointer\n",
        "earlystopping = EarlyStopping(monitor = 'val_loss', patience = 20, verbose = 1, mode = 'min')\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath = 'ResUNet_mask.hdf5', verbose = 1, save_best_only= True)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXq93HDkMg_",
        "outputId": "01c0cf74-0490-451d-b856-88c7f34e9e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(training_gen, epochs = 50, validation_data= validation_gen, callbacks = [earlystopping, checkpoint])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.8276 - tversky: 0.2216 \n",
            "Epoch 00001: val_loss improved from inf to 0.82022, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 1124s 16s/step - loss: 0.8276 - tversky: 0.2216 - val_loss: 0.8202 - val_tversky: 0.2319\n",
            "Epoch 2/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.4935 - tversky: 0.6064\n",
            "Epoch 00002: val_loss improved from 0.82022 to 0.71426, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 31s 425ms/step - loss: 0.4935 - tversky: 0.6064 - val_loss: 0.7143 - val_tversky: 0.3613\n",
            "Epoch 3/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3884 - tversky: 0.7145\n",
            "Epoch 00003: val_loss improved from 0.71426 to 0.45119, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 195ms/step - loss: 0.3884 - tversky: 0.7145 - val_loss: 0.4512 - val_tversky: 0.6525\n",
            "Epoch 4/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3384 - tversky: 0.7623\n",
            "Epoch 00004: val_loss did not improve from 0.45119\n",
            "72/72 [==============================] - 13s 176ms/step - loss: 0.3384 - tversky: 0.7623 - val_loss: 0.4636 - val_tversky: 0.6408\n",
            "Epoch 5/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2967 - tversky: 0.8004\n",
            "Epoch 00005: val_loss did not improve from 0.45119\n",
            "72/72 [==============================] - 13s 176ms/step - loss: 0.2967 - tversky: 0.8004 - val_loss: 0.5549 - val_tversky: 0.5425\n",
            "Epoch 6/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2933 - tversky: 0.8028\n",
            "Epoch 00006: val_loss improved from 0.45119 to 0.35729, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 196ms/step - loss: 0.2933 - tversky: 0.8028 - val_loss: 0.3573 - val_tversky: 0.7456\n",
            "Epoch 7/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2687 - tversky: 0.8248\n",
            "Epoch 00007: val_loss improved from 0.35729 to 0.26354, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 201ms/step - loss: 0.2687 - tversky: 0.8248 - val_loss: 0.2635 - val_tversky: 0.8305\n",
            "Epoch 8/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2528 - tversky: 0.8382\n",
            "Epoch 00008: val_loss improved from 0.26354 to 0.25107, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 199ms/step - loss: 0.2528 - tversky: 0.8382 - val_loss: 0.2511 - val_tversky: 0.8397\n",
            "Epoch 9/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2335 - tversky: 0.8548\n",
            "Epoch 00009: val_loss improved from 0.25107 to 0.24319, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 198ms/step - loss: 0.2335 - tversky: 0.8548 - val_loss: 0.2432 - val_tversky: 0.8462\n",
            "Epoch 10/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2266 - tversky: 0.8604\n",
            "Epoch 00010: val_loss did not improve from 0.24319\n",
            "72/72 [==============================] - 13s 180ms/step - loss: 0.2266 - tversky: 0.8604 - val_loss: 0.2773 - val_tversky: 0.8185\n",
            "Epoch 11/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2118 - tversky: 0.8726\n",
            "Epoch 00011: val_loss improved from 0.24319 to 0.23510, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 200ms/step - loss: 0.2118 - tversky: 0.8726 - val_loss: 0.2351 - val_tversky: 0.8538\n",
            "Epoch 12/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2252 - tversky: 0.8618\n",
            "Epoch 00012: val_loss improved from 0.23510 to 0.23185, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 200ms/step - loss: 0.2252 - tversky: 0.8618 - val_loss: 0.2319 - val_tversky: 0.8564\n",
            "Epoch 13/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2059 - tversky: 0.8774\n",
            "Epoch 00013: val_loss improved from 0.23185 to 0.22906, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 201ms/step - loss: 0.2059 - tversky: 0.8774 - val_loss: 0.2291 - val_tversky: 0.8596\n",
            "Epoch 14/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1894 - tversky: 0.8902\n",
            "Epoch 00014: val_loss improved from 0.22906 to 0.20785, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 197ms/step - loss: 0.1894 - tversky: 0.8902 - val_loss: 0.2079 - val_tversky: 0.8761\n",
            "Epoch 15/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1722 - tversky: 0.9036\n",
            "Epoch 00015: val_loss improved from 0.20785 to 0.20393, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 197ms/step - loss: 0.1722 - tversky: 0.9036 - val_loss: 0.2039 - val_tversky: 0.8794\n",
            "Epoch 16/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1665 - tversky: 0.9079\n",
            "Epoch 00016: val_loss did not improve from 0.20393\n",
            "72/72 [==============================] - 13s 181ms/step - loss: 0.1665 - tversky: 0.9079 - val_loss: 0.2150 - val_tversky: 0.8697\n",
            "Epoch 17/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1693 - tversky: 0.9056\n",
            "Epoch 00017: val_loss did not improve from 0.20393\n",
            "72/72 [==============================] - 13s 181ms/step - loss: 0.1693 - tversky: 0.9056 - val_loss: 0.2150 - val_tversky: 0.8694\n",
            "Epoch 18/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1568 - tversky: 0.9150\n",
            "Epoch 00018: val_loss did not improve from 0.20393\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1568 - tversky: 0.9150 - val_loss: 0.2068 - val_tversky: 0.8770\n",
            "Epoch 19/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1499 - tversky: 0.9198\n",
            "Epoch 00019: val_loss improved from 0.20393 to 0.18172, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 15s 202ms/step - loss: 0.1499 - tversky: 0.9198 - val_loss: 0.1817 - val_tversky: 0.8958\n",
            "Epoch 20/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1473 - tversky: 0.9218\n",
            "Epoch 00020: val_loss did not improve from 0.18172\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1473 - tversky: 0.9218 - val_loss: 0.1884 - val_tversky: 0.8915\n",
            "Epoch 21/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1390 - tversky: 0.9276\n",
            "Epoch 00021: val_loss improved from 0.18172 to 0.17958, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 201ms/step - loss: 0.1390 - tversky: 0.9276 - val_loss: 0.1796 - val_tversky: 0.8981\n",
            "Epoch 22/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1371 - tversky: 0.9289\n",
            "Epoch 00022: val_loss did not improve from 0.17958\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1371 - tversky: 0.9289 - val_loss: 0.2071 - val_tversky: 0.8771\n",
            "Epoch 23/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1488 - tversky: 0.9203\n",
            "Epoch 00023: val_loss did not improve from 0.17958\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1488 - tversky: 0.9203 - val_loss: 0.2634 - val_tversky: 0.8310\n",
            "Epoch 24/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1428 - tversky: 0.9248\n",
            "Epoch 00024: val_loss improved from 0.17958 to 0.17578, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 14s 201ms/step - loss: 0.1428 - tversky: 0.9248 - val_loss: 0.1758 - val_tversky: 0.9010\n",
            "Epoch 25/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1289 - tversky: 0.9346\n",
            "Epoch 00025: val_loss did not improve from 0.17578\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1289 - tversky: 0.9346 - val_loss: 0.1814 - val_tversky: 0.8968\n",
            "Epoch 26/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1259 - tversky: 0.9365\n",
            "Epoch 00026: val_loss improved from 0.17578 to 0.16959, saving model to ResUNet_mask.hdf5\n",
            "72/72 [==============================] - 15s 201ms/step - loss: 0.1259 - tversky: 0.9365 - val_loss: 0.1696 - val_tversky: 0.9051\n",
            "Epoch 27/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1236 - tversky: 0.9381\n",
            "Epoch 00027: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1236 - tversky: 0.9381 - val_loss: 0.1723 - val_tversky: 0.9037\n",
            "Epoch 28/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1207 - tversky: 0.9401\n",
            "Epoch 00028: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1207 - tversky: 0.9401 - val_loss: 0.1768 - val_tversky: 0.9006\n",
            "Epoch 29/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1180 - tversky: 0.9418\n",
            "Epoch 00029: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1180 - tversky: 0.9418 - val_loss: 0.2015 - val_tversky: 0.8804\n",
            "Epoch 30/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1143 - tversky: 0.9443\n",
            "Epoch 00030: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1143 - tversky: 0.9443 - val_loss: 0.1767 - val_tversky: 0.9005\n",
            "Epoch 31/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1115 - tversky: 0.9461\n",
            "Epoch 00031: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1115 - tversky: 0.9461 - val_loss: 0.1735 - val_tversky: 0.9026\n",
            "Epoch 32/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1119 - tversky: 0.9458\n",
            "Epoch 00032: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1119 - tversky: 0.9458 - val_loss: 0.2086 - val_tversky: 0.8738\n",
            "Epoch 33/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1096 - tversky: 0.9473\n",
            "Epoch 00033: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1096 - tversky: 0.9473 - val_loss: 0.2028 - val_tversky: 0.8796\n",
            "Epoch 34/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1077 - tversky: 0.9486\n",
            "Epoch 00034: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1077 - tversky: 0.9486 - val_loss: 0.1865 - val_tversky: 0.8930\n",
            "Epoch 35/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1044 - tversky: 0.9506\n",
            "Epoch 00035: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.1044 - tversky: 0.9506 - val_loss: 0.1770 - val_tversky: 0.9000\n",
            "Epoch 36/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1028 - tversky: 0.9516\n",
            "Epoch 00036: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.1028 - tversky: 0.9516 - val_loss: 0.1700 - val_tversky: 0.9057\n",
            "Epoch 37/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1013 - tversky: 0.9525\n",
            "Epoch 00037: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.1013 - tversky: 0.9525 - val_loss: 0.1801 - val_tversky: 0.8973\n",
            "Epoch 38/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0994 - tversky: 0.9538\n",
            "Epoch 00038: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.0994 - tversky: 0.9538 - val_loss: 0.1884 - val_tversky: 0.8907\n",
            "Epoch 39/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0984 - tversky: 0.9544\n",
            "Epoch 00039: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.0984 - tversky: 0.9544 - val_loss: 0.2095 - val_tversky: 0.8747\n",
            "Epoch 40/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0975 - tversky: 0.9549\n",
            "Epoch 00040: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.0975 - tversky: 0.9549 - val_loss: 0.1862 - val_tversky: 0.8927\n",
            "Epoch 41/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0965 - tversky: 0.9556\n",
            "Epoch 00041: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.0965 - tversky: 0.9556 - val_loss: 0.1772 - val_tversky: 0.8990\n",
            "Epoch 42/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0941 - tversky: 0.9571\n",
            "Epoch 00042: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.0941 - tversky: 0.9571 - val_loss: 0.1799 - val_tversky: 0.8975\n",
            "Epoch 43/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0937 - tversky: 0.9573\n",
            "Epoch 00043: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.0937 - tversky: 0.9573 - val_loss: 0.1965 - val_tversky: 0.8853\n",
            "Epoch 44/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0939 - tversky: 0.9571\n",
            "Epoch 00044: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 14s 188ms/step - loss: 0.0939 - tversky: 0.9571 - val_loss: 0.1732 - val_tversky: 0.9032\n",
            "Epoch 45/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0903 - tversky: 0.9593\n",
            "Epoch 00045: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 14s 190ms/step - loss: 0.0903 - tversky: 0.9593 - val_loss: 0.1989 - val_tversky: 0.8836\n",
            "Epoch 46/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0886 - tversky: 0.9604\n",
            "Epoch 00046: val_loss did not improve from 0.16959\n",
            "72/72 [==============================] - 14s 193ms/step - loss: 0.0886 - tversky: 0.9604 - val_loss: 0.1965 - val_tversky: 0.8856\n",
            "Epoch 00046: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgpaLH-XkMl1"
      },
      "source": [
        "import json\n",
        "model_json = model.to_json()\n",
        "with open('ResUNet_architecture.json','w') as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBk6fRNBkMoX"
      },
      "source": [
        ""
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLJC4MwgkMq5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ47UeiikMtj"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bXJEKYjkMwL"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DffbJpi7kMyn"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x42rwjJqkM1I"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH64LkbpkM3t"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLGyRywPkM6K"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0sTOAAOkM83"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc7eGP5AkM_j"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5e-VXKmkNB2"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBzV3JBxkNEY"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r1bZ-xDkNGz"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwnqIrE2kNJr"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i2DQ9QekNMD"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ay5UJokNOu"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBSDAIhkNRG"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skZvCNN1kNTX"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW9lnGL2kNVx"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_zyFYL7kNYB"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PywJlAlGkNaj"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzeaHterkNdC"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvPCTYkJkNfr"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}
